{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "855f25b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing important libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f60345",
   "metadata": {},
   "source": [
    "# Basic Scrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4e4b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://webscraper.io/test-sites/e-commerce/allinone/computers' #storing the url\n",
    "page = requests.get(url) #getting the access of website\n",
    "soup = BeautifulSoup(page.text,'lxml') # getting the html format\n",
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c693801e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.header.p.string) #to get the content of any tag\n",
    "print(soup.header.a.attrs) #to get all the attribute as a dictionary\n",
    "\n",
    "#to finds the first occurance\n",
    "print(soup.find('h2'))\n",
    "print(soup.find('div', class_ = 'navbar-header'))\n",
    "print(soup.find('div',{'class': 'navbar-brand' }))\n",
    "\n",
    "#to find all the occurances\n",
    "print(soup.find_all('a', class_ = 'title'))\n",
    "soup.find_all(['p','a'])\n",
    "\n",
    "print(soup.find_all(id = True)) #to find by id\n",
    "print(soup.find_all(string = 'Asus')) #to find by string exact match\n",
    "print(soup.find_all(string = re.compile('Asus'))) # to find similar match\n",
    "print(soup.find_all('p',class_ = re.compile('pull'),limit = 2)) #to limit scrapping\n",
    "print(soup.find_all(string = [re.compile('Asus'),re.compile('White')])) #to find multiple items\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c04fe5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a dataframe\n",
    "product_name = soup.find_all('a',class_ = 'title')\n",
    "price = soup.find_all('h4',class_ = 'pull-right price')\n",
    "reviews = soup.find_all('p', class_ = 'pull-right')\n",
    "description = soup.find_all('p', class_ = 'description')\n",
    "\n",
    "product_list = price_list = review_list = description_list = [] #making empty list\n",
    "\n",
    "for i,j,k,l in zip(product_name, price, reviews, description): #looping over all the tags\n",
    "    name = i.text #extracting the text\n",
    "    product_list.append(name) # storing in the list\n",
    "    \n",
    "    p = j.text\n",
    "    price_list.append(p)\n",
    "    \n",
    "    r = k.text\n",
    "    review_list.append(r)\n",
    "    \n",
    "    d = l.text\n",
    "    description_list.append(d)\n",
    "    \n",
    "table = pd.DataFrame({'Product':product_list,'Price':price_list,'Description':description_list,'Review':review_list})\n",
    "table #final dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af87dfbd",
   "metadata": {},
   "source": [
    "# Table Scrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b08aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.nfl.com/standings/league/2022/REG'\n",
    "page = requests.get(url)\n",
    "soup = BeautifulSoup(page.text,'lxml')\n",
    "nested = soup.find('table', {'summary': 'Standings - Detailed View'}) # getting only the table html tag\n",
    "nested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d299e075",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_name = []\n",
    "for i in nested.find_all('th'):\n",
    "    name = i.text.strip() \n",
    "    column_name.append(name) # getting the header of the table\n",
    "column_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "636df906",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NFL Team</th>\n",
       "      <th>W</th>\n",
       "      <th>L</th>\n",
       "      <th>T</th>\n",
       "      <th>PCT</th>\n",
       "      <th>PF</th>\n",
       "      <th>PA</th>\n",
       "      <th>Net Pts</th>\n",
       "      <th>Home</th>\n",
       "      <th>Road</th>\n",
       "      <th>Div</th>\n",
       "      <th>Pct</th>\n",
       "      <th>Conf</th>\n",
       "      <th>Pct</th>\n",
       "      <th>Non-Conf</th>\n",
       "      <th>Strk</th>\n",
       "      <th>Last 5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [NFL Team, W, L, T, PCT, PF, PA, Net Pts, Home, Road, Div, Pct, Conf, Pct, Non-Conf, Strk, Last 5]\n",
       "Index: []"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nfl = pd.DataFrame(columns = column_name)\n",
    "nfl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b15ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the data of all the rows\n",
    "for i in nested.find_all('tr')[1:]:\n",
    "    first_data = i.find('td').find('div', class_ = 'd3-o-club-fullname').text.strip() # getting all the team name\n",
    "    row_data = i.find_all('td')[1:]\n",
    "    data = [j.text.strip() for j in row_data] #getting the data\n",
    "    data.insert(0,first_data)\n",
    "    length = len(nfl)\n",
    "    nfl.loc[length] = data\n",
    "nfl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a648a3",
   "metadata": {},
   "source": [
    "# Airbnb Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69b4e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#searching for hotels in goa\n",
    "url = 'https://www.airbnb.co.in/s/Goa/homes?tab_id=home_tab&refinement_paths%5B%5D=%2Fhomes&flexible_trip_lengths%5B%5D=one_week&price_filter_input_type=0&price_filter_num_nights=5&query=Goa&place_id=ChIJQbc2YxC6vzsRkkDzYv-H-Oo&date_picker_type=calendar&checkin=2022-12-24&checkout=2023-01-02&flexible_date_search_filter_type=0&source=structured_search_input_header&search_type=autocomplete_click'\n",
    "page = requests.get(url)\n",
    "soup = BeautifulSoup(page.text, 'lxml')\n",
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152f9bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#making the dataframe\n",
    "air_df = pd.DataFrame({'link':[''],'title':[''],'name':[''],'detail':[''],'rating':[''],'price':['']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce27492e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting from multiple pages\n",
    "while True:\n",
    "    posting = soup.find_all('div', class_= 'cy5jw6o dir dir-ltr') # storing all the posting\n",
    "    for post in posting:\n",
    "        try:\n",
    "            link = post.find('a', class_ = 'bn2bl2p dir dir-ltr').get('href') #getting the link\n",
    "            full_link = 'https://www.airbnb.co.in' + link # appending the full link\n",
    "            title = post.find('div', class_ = 't1jojoys dir dir-ltr').text\n",
    "            name = post.find('span', class_ = 't6mzqp7 dir dir-ltr').text\n",
    "            detail = post.find_all('div', class_ = 'f15liw5s s1cjsi4j dir dir-ltr')[1].text\n",
    "            rating = post.find('span', class_ = 'r1dxllyb dir dir-ltr').text\n",
    "            try: \n",
    "                price = post.find('span', class_ = '_tyxjp1').text # two price classes are there so try which one is present\n",
    "            except:\n",
    "                price = post.find('span', class_ = '_1y74zjx').text\n",
    "            #finally appending into the dataframe    \n",
    "            air_df = air_df.append({'link':full_link,'title':title,'name':name,'detail':detail,'rating':rating,'price':price}, ignore_index = True)\n",
    "        except: # if any tag doesn't have the value then just ignore that posting and pass\n",
    "            pass\n",
    "    next_page = soup.find('a',{'aria-label':'Next'}).get('href') #next page link\n",
    "    full_link = 'https://www.airbnb.co.in' + next_page\n",
    "    page = requests.get(full_link)\n",
    "    soup = BeautifulSoup(page.text, 'lxml') # getting the next page html tags   \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a55bd4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
